waves.matcher {
  # Matcher's account address
  account = ""

  # Matcher's directories
  matcher-directory = ${waves.directory}"/matcher"
  data-directory = ${waves.matcher.matcher-directory}"/data"
  journal-directory = ${waves.matcher.matcher-directory}"/journal"
  snapshots-directory = ${waves.matcher.matcher-directory}"/snapshots"

  # Matcher REST API bind address
  bind-address = "127.0.0.1"

  # Matcher REST API port
  port = 6886

  # Base fee for the exchange transaction
  exchange-tx-base-fee = 300000

  # Settings for matcher's fee in order
  order-fee {
    # Dynamic fee in some asset from the predefined list or
    # fixed asset and fee or
    # percent fee in asset of the pair
    mode = "dynamic" # | "fixed" | "percent"

    # In this mode matcher charges additional fee for its
    # account script and scripts of the assets of the pair (if exists).
    # Matcher accepts fee in several assets which can be obtained by
    # the following REST request: GET /matcher/settings/rates
    # Fee is charged according to the asset rate (its cost in Waves)
    dynamic {
      # Absolute
      base-fee = 300000
    }

    fixed {
      # Fixed fee asset
      asset = "WAVES" # | "some issued asset (base58)"

      # Minimum allowed order fee for fixed mode
      min-fee = 300000
    }

    percent {
      # Asset type for fee
      asset-type = "amount" # | "price" | "spending" | "receiving"

      # In percents
      min-fee = 0.1
    }
  }

  # Price and fee deviations (in percents)
  max-price-deviations {
    # Enable/disable deviations checks
    enable = no
    # Max price deviation IN FAVOR of the client
    profit = 1000000
    # Max price deviation AGAINST the client
    loss = 1000000
    # Max fee deviation from the market price
    fee = 1000000
  }

  # Restrictions for the orders. Empty list means that there are no restrictions on the orders
  #
  #  Default values for the pairs:
  #
  #   min-amount = 0.00000001,
  #   max-amount = 1000000000
  #   step-size  = 0.00000001
  #   min-price  = 0.00000001,
  #   max-price  = 1000000
  #   tick-size  = 0.00000001
  #   merge-small-prices = no
  #
  # Example:
  #
  # order-restrictions = [
  #   {
  #     pair = "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS",
  #     min-amount = 0.001,
  #     max-amount = 1000000
  #     step-size = 0.001
  #     min-price = 0.001,
  #     max-price = 100000,
  #     tick-size = 0.002,
  #     merge-small-prices = yes
  #   },
  #   ...
  # ]
  order-restrictions = []

  # Disable charging of additional fee for new orders from scripted accounts or for smart assets
  disable-extra-fee-for-script = no

  # Postgres connection settings
  postgres {
    server-name = "localhost"
    port-number = 5435
    user = "user"
    password = "user"
    data-source-class-name = "org.postgresql.ds.PGSimpleDataSource"
  }

  # History of the orders and their events, uses Postgres
  #
  #  Defaults:
  #
  #  batch-linger-ms = 1000
  #  batch-entries   = 10000
  #
  order-history {
    # Enable/disable order history
    enabled = no

    # Time for delay between batches
    orders-batch-linger-ms = 1000
    # Etries count for the batch
    orders-batch-entries = 10000

    events-batch-linger-ms = 1000
    events-batch-entries = 10000
  }

  # Snapshots creation interval (in events)
  snapshots-interval = 1000000

  # During recovery determine the offset to start:
  # If the oldest snapshot has 2025331 offset, we start from startOldestOffset = truncate(2025331 / snapshots-interval * snapshots-interval) = 2000000.
  # This option allows to limit events from the newest snapshot also. For example, the newest snapshot was done at 3092345. startNewestOffset = 3092345 - limit-events-during-recovery
  # If this option is defined, the maximum wins = max(startOldestOffset, startNewestOffset), otherwise we start from startOldestOffset
  # limit-events-during-recovery = 2000000

  # Maximum time to recover all order books from snapshots
  snapshots-loading-timeout = 10m

  # Maximum time to recover events those observed at start
  start-events-processing-timeout = 20m

  # Maximum time to process recovered events by order books
  order-books-recovering-timeout = 10m

  # Maximum allowed amount of orders retrieved via REST
  rest-order-limit = 100

  # Base assets used as price assets
  price-assets: []

  # Blacklisted assets id
  blacklisted-assets: []

  # Blacklisted assets name
  blacklisted-names: []

  # Blacklisted addresses
  blacklisted-addresses: []

  # * yes - only "allowed-asset-pairs" are allowed to trade. Other pairs are blacklisted.
  # * no  - "allowed-asset-pairs" are permitted to trade. If a pair is not in "allowed-asset-pairs",
  #         it's checked by "blacklisted-assets" and "blacklisted-names".
  white-list-only = no

  # Example:
  # allowed-asset-pairs = [
  #  "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS"
  # ]
  allowed-asset-pairs: []

  # Set of allowed order versions
  allowed-order-versions = [1, 2]

  # Cache for /matcher/orderbook/{amountAsset}/{priceAsset}?depth=N
  order-book-snapshot-http-cache {
    # A timeout to store cache
    cache-timeout = 5s

    # Cache for these depths. When ?depth=3 is requested, returned a cache for depth of 10
    depth-ranges = [10, 100]
  }

  # Interval to buffer balance changes before process them
  balance-watching-buffer-interval = 5s

  # Queue for events (order was added, order was cancelled)
  events-queue {
    # Store events locally in LevelDB
    type = "local" # Other possible values: kafka

    local {
      # If "no" - no events will be written to the queue. Useful for debugging
      enable-storing = yes

      # Interval between reads from the disk
      polling-interval = 20ms

      # Max elements per poll
      max-elements-per-poll = 100

      # Clean old records before start consuming
      clean-before-consume = yes
    }

    kafka {
      # Kafka servers in format: host1:port1,host2:port2,...
      servers = ""

      # Where events should be written and read from
      topic = "dex-events"

      # For different matchers connected to the same topic should be different groups
      group = "0"

      # Consumer-related settings
      consumer {
        # Buffer for polled events
        buffer-size = 100

        # Recovery settings (see akka.stream.scaladsl.Restart for detailed information)

        # Initial duration before consumer start after fail
        min-backoff = 3s

        # Maximum duration before consumer start after fail
        max-backoff = 10s
      }

      # Producer-related settings
      producer {
        # If "no" - no events will be written to the queue. Useful for debugging
        enable = yes

        # Buffer for pushed events
        buffer-size = 100
      }
    }
  }

  # Settings for transaction broadcaster
  exchange-transaction-broadcast {
    # Broadcast exchange transactions until they are confirmed by blockchain.
    # If "no", a transaction will be broadcasted once.
    broadcast-until-confirmed = no

    # When broadcast-until-confirmed = yes
    # * Bettween checks;
    # * A transaction will not be sent more frequently than this interval.
    interval = 1 minute

    # When broadcast-until-confirmed = yes
    # Not sended transaction:
    # * Will be removed from queue after this timeout;
    # * A warning will be loggeg.
    max-pending-time = 15 minutes
  }
}

# WARNING: No user-configurable settings below this line.

waves.matcher {
  # Timeout for REST API responses from actors.
  # To change a timeout for all REST API responses, change this option and akka.http.server.request-timeout
  actor-response-timeout = ${akka.http.server.request-timeout}

  snapshot-store {
    class = "com.wavesplatform.matcher.MatcherSnapshotStore"
    plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
    stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"
    dir = ${waves.matcher.snapshots-directory}
  }
}

akka {
  # Without this option, Matcher can't make snapshots.
  # For better support of shutdown process, implement an additional step in a coordinated shutdown:
  # https://doc.akka.io/docs/akka/2.5/actors.html?language=scala#coordinated-shutdown
  jvm-shutdown-hooks = off

  actor {
    allow-java-serialization = off
    guardian-supervisor-strategy = "com.wavesplatform.actor.RootActorSystem$EscalatingStrategy"
    serializers.matcher = "com.wavesplatform.matcher.model.EventSerializers"
    serialization-bindings {
      "com.wavesplatform.matcher.model.Events$Event" = matcher
      "com.wavesplatform.matcher.market.OrderBookActor$Snapshot" = matcher
      "com.wavesplatform.matcher.market.MatcherActor$OrderBookCreated" = matcher
      "com.wavesplatform.matcher.market.MatcherActor$Snapshot" = matcher
    }

    deployment {
      "/exchange-transaction-broadcast" {
        dispatcher = "akka.actor.broadcast-dispatcher"
      }
      "/addresses/history-router/*" {
        dispatcher = "akka.actor.orders-history-dispatcher"
      }
    }

    broadcast-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.fixed-pool-size = 1
      throughput = 1
    }

    orders-history-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor.fixed-pool-size = 1
      throughput = 1
    }
  }

  persistence {
    journal {
      plugin = akka.persistence.journal.leveldb
      leveldb {
        dir = ${waves.matcher.journal-directory}
        native = on
      }
    }
    snapshot-store.plugin = waves.matcher.snapshot-store
  }

  kafka {
    default-dispatcher.thread-pool-executor.fixed-pool-size = 4

    consumer {
      # Tuning property of scheduled polls.
      # Controls the interval from one scheduled poll to the next.
      poll-interval = 30ms

      # Tuning property of the `KafkaConsumer.poll` parameter.
      # Note that non-zero value means that the thread that
      # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
      poll-timeout = 100ms

      # The stage will await outstanding offset commit requests before
      # shutting down, but if that takes longer than this timeout it will
      # stop forcefully.
      stop-timeout = 30s

      # Duration to wait for `KafkaConsumer.close` to finish.
      close-timeout = 20s

      # If offset commit requests are not completed within this timeout
      # the returned Future is completed `CommitTimeoutException`.
      commit-timeout = 15s

      # If commits take longer than this time a warning is logged
      commit-time-warning = 1s

      # If for any reason `KafkaConsumer.poll` blocks for longer than the configured
      # poll-timeout then it is forcefully woken up with `KafkaConsumer.wakeup`.
      # The KafkaConsumerActor will throw
      # `org.apache.kafka.common.errors.WakeupException` which will be ignored
      # until `max-wakeups` limit gets exceeded.
      wakeup-timeout = 1m

      # After exceeding maximum wakeups the consumer will stop and the stage will fail.
      # Setting it to 0 will let it ignore the wakeups and try to get the polling done forever.
      max-wakeups = 0

      # If set to a finite duration, the consumer will re-send the last committed offsets periodically
      # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
      commit-refresh-interval = infinite

      # If enabled, log stack traces before waking up the KafkaConsumer to give
      # some indication why the KafkaConsumer is not honouring the `poll-timeout`
      wakeup-debug = false

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the KafkaConsumerActor. Some blocking may occur.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
      # can be defined in this configuration section.
      kafka-clients {
        bootstrap.servers = ${waves.matcher.events-queue.kafka.servers}
        group.id = ${waves.matcher.events-queue.kafka.group}
        auto.offset.reset = "earliest"
        enable.auto.commit = false
        session.timeout.ms = 10000
        max.poll.interval.ms = 11000
        max.poll.records = 100 # Should be <= ${waves.matcher.events-queue.kafka.consumer.buffer-size}
      }

      # Time to wait for pending requests when a partition is closed
      wait-close-partition = 500ms

      # Limits the query to Kafka for a topic's position
      position-timeout = 5s

      # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
      # call to Kafka's API
      offset-for-times-timeout = 5s

      # Timeout for akka.kafka.Metadata requests
      # This value is used instead of Kafka's default from `default.api.timeout.ms`
      # which is 1 minute.
      metadata-request-timeout = 20s
    }

    producer {
      # Tuning parameter of how many sends that can run in parallel.
      parallelism = 100

      # Duration to wait for `KafkaConsumer.close` to finish.
      close-timeout = 60s

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the producer stages. Some blocking may occur.
      # When this value is empty, the dispatcher configured for the stream
      # will be used.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
      eos-commit-interval = 100ms

      # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
      # can be defined in this configuration section.
      kafka-clients {
        bootstrap.servers = ${waves.matcher.events-queue.kafka.servers}

        acks = all

        # Buffer messages into a batch for this duration
        linger.ms = 5

        # Maximum size for batch
        batch.size = 16384

        # To guarantee the order
        max.in.flight.requests.per.connection = 1

        compression.type = "none"
      }
    }
  }
}

include "dex-base.conf"
