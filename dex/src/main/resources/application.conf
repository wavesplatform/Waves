waves.matcher {
  # Timeout for REST API responses from actors.
  # To change a timeout for all REST API responses, change this option and akka.http.server.request-timeout
  actor-response-timeout = ${akka.http.server.request-timeout}

  snapshot-store {
    class = "com.wavesplatform.matcher.MatcherSnapshotStore"
    plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
    stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"
    dir = ${waves.matcher.snapshots-directory}
  }
}

akka {
  # Without this option, Matcher can't make snapshots.
  # For better support of shutdown process, implement an additional step in a coordinated shutdown:
  # https://doc.akka.io/docs/akka/2.5/actors.html?language=scala#coordinated-shutdown
  jvm-shutdown-hooks = off

  actor {
    allow-java-serialization = off
    guardian-supervisor-strategy = "com.wavesplatform.actor.RootActorSystem$EscalatingStrategy"
    serializers.matcher = "com.wavesplatform.matcher.model.EventSerializers"
    serialization-bindings {
      "com.wavesplatform.matcher.model.Events$Event" = matcher
      "com.wavesplatform.matcher.market.OrderBookActor$Snapshot" = matcher
      "com.wavesplatform.matcher.market.MatcherActor$OrderBookCreated" = matcher
      "com.wavesplatform.matcher.market.MatcherActor$Snapshot" = matcher
    }
  }

  deployment {
    "/history-router/orders-history" {
      dispatcher = "akka.actor.orders-history-dispatcher"
    }
    "/history-router/events-history" {
      dispatcher = "akka.actor.orders-history-dispatcher"
    }
  }

  orders-history-dispatcher {
    type = "Dispatcher"
    executor = "thread-pool-executor"
    thread-pool-executor.fixed-pool-size = 1
    throughput = 1
  }

  persistence {
    journal {
      plugin = akka.persistence.journal.leveldb
      leveldb {
        dir = ${waves.matcher.journal-directory}
        native = on
      }
    }
    snapshot-store.plugin = waves.matcher.snapshot-store
  }

  kafka {
    default-dispatcher.thread-pool-executor.fixed-pool-size = 4

    consumer {
      # Tuning property of scheduled polls.
      # Controls the interval from one scheduled poll to the next.
      poll-interval = 30ms

      # Tuning property of the `KafkaConsumer.poll` parameter.
      # Note that non-zero value means that the thread that
      # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
      poll-timeout = 100ms

      # The stage will await outstanding offset commit requests before
      # shutting down, but if that takes longer than this timeout it will
      # stop forcefully.
      stop-timeout = 30s

      # Duration to wait for `KafkaConsumer.close` to finish.
      close-timeout = 20s

      # If offset commit requests are not completed within this timeout
      # the returned Future is completed `CommitTimeoutException`.
      commit-timeout = 15s

      # If commits take longer than this time a warning is logged
      commit-time-warning = 1s

      # If for any reason `KafkaConsumer.poll` blocks for longer than the configured
      # poll-timeout then it is forcefully woken up with `KafkaConsumer.wakeup`.
      # The KafkaConsumerActor will throw
      # `org.apache.kafka.common.errors.WakeupException` which will be ignored
      # until `max-wakeups` limit gets exceeded.
      wakeup-timeout = 1m

      # After exceeding maximum wakeups the consumer will stop and the stage will fail.
      # Setting it to 0 will let it ignore the wakeups and try to get the polling done forever.
      max-wakeups = 0

      # If set to a finite duration, the consumer will re-send the last committed offsets periodically
      # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
      commit-refresh-interval = infinite

      # If enabled, log stack traces before waking up the KafkaConsumer to give
      # some indication why the KafkaConsumer is not honouring the `poll-timeout`
      wakeup-debug = false

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the KafkaConsumerActor. Some blocking may occur.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
      # can be defined in this configuration section.
      kafka-clients {
        bootstrap.servers = ${waves.matcher.events-queue.kafka.servers}
        group.id = ${waves.matcher.events-queue.kafka.group}
        auto.offset.reset = "earliest"
        enable.auto.commit = false
        session.timeout.ms = 10000
        max.poll.interval.ms = 11000
        max.poll.records = 100 # Should be <= ${waves.matcher.events-queue.kafka.consumer.buffer-size}
      }

      # Time to wait for pending requests when a partition is closed
      wait-close-partition = 500ms

      # Limits the query to Kafka for a topic's position
      position-timeout = 5s

      # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
      # call to Kafka's API
      offset-for-times-timeout = 5s

      # Timeout for akka.kafka.Metadata requests
      # This value is used instead of Kafka's default from `default.api.timeout.ms`
      # which is 1 minute.
      metadata-request-timeout = 20s
    }

    producer {
      # Tuning parameter of how many sends that can run in parallel.
      parallelism = 100

      # Duration to wait for `KafkaConsumer.close` to finish.
      close-timeout = 60s

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the producer stages. Some blocking may occur.
      # When this value is empty, the dispatcher configured for the stream
      # will be used.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
      eos-commit-interval = 100ms

      # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
      # can be defined in this configuration section.
      kafka-clients {
        bootstrap.servers = ${waves.matcher.events-queue.kafka.servers}

        acks = all

        # Buffer messages into a batch for this duration
        linger.ms = 5

        # Maximum size for batch
        batch.size = 16384

        # To guarantee the order
        max.in.flight.requests.per.connection = 1

        compression.type = "none"
      }
    }
  }
}

include "dex-base.conf"
