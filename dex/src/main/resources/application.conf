waves.matcher {
  # Matcher's account address
  account = ""

  # Matcher REST API bind address
  bind-address = "127.0.0.1"

  # Matcher REST API port
  port = 6886

  # Timeout for REST API responses from actors.
  # To change a timeout for all REST API responses, change this option and akka.http.server.request-timeout
  actor-response-timeout = ${akka.http.server.request-timeout}

  # Settings for matcher's fee in order
  order-fee {
    # Standart fee in waves or fixed asset and fee or percent fee in asset of the pair
    mode = "fixed-waves" # | "fixed" | "percent"

    # Has additional change for matcher and assets scripts
    fixed-waves {
      # Absolute
      base-fee = 300000
    }

    fixed {
      # Fixed fee asset
      asset-id = "8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS" # | "another asset != Waves"

      # Minimum allowed order fee for fixed mode
      min-fee = 300000
    }

    percent {
      # Asset type for fee
      asset-type = "amount" # | "price" | "spending" | "receiving"

      # In percents
      min-fee = 0.1
    }
  }

  # Matcher's directories
  matcher-directory = ${waves.directory}"/matcher"
  data-directory = ${waves.matcher.matcher-directory}"/data"
  journal-directory = ${waves.matcher.matcher-directory}"/journal"
  snapshots-directory = ${waves.matcher.matcher-directory}"/snapshots"

  # Snapshots creation interval (in events)
  snapshots-interval = 1000000

  # Make snapshots after recovery at start
  make-snapshots-at-start = no

  # Maximum time to recover all order books from snapshots
  snapshots-loading-timeout = 10m

  # Maximum time to recover events those observed at start
  start-events-processing-timeout = 20m

  # Maximum allowed amount of orders retrieved via REST
  rest-order-limit = 100

  # Base assets used as price assets
  price-assets: []

  # Blacklisted assets id
  blacklisted-assets: []

  # Blacklisted assets name
  blacklisted-names: []

  # Blacklisted addresses
  blacklisted-addresses: []

  # Cache for /matcher/orderbook/{amountAsset}/{priceAsset}?depth=N
  order-book-snapshot-http-cache {
    # A timeout to store cache
    cache-timeout = 5s

    # Cache for these depths. When ?depth=3 is requested, returned a cache for depth of 10
    depth-ranges = [10, 100]
  }

  # Interval to buffer balance changes before process them
  balance-watching-buffer-interval = 5s

  # Queue for events (order was added, order was cancelled)
  events-queue {
    # Store events locally in LevelDB
    type = "local" # Other possible values: kafka

    local {
      # Interval between reads from the disk
      polling-interval = 20ms

      # Max elements per poll
      max-elements-per-poll = 100

      # Clean old records before start consuming
      clean-before-consume = yes
    }

    kafka {
      # Where events should be written and read from
      topic = "dex-events"

      # Consumer-related settings
      consumer {
        # Buffer for polled events
        buffer-size = 100

        # Recovery settings (see akka.stream.scaladsl.Restart for detailed information)

        # Initial duration before consumer start after fail
        min-backoff = 3s

        # Maximum duration before consumer start after fail
        max-backoff = 10s
      }

      # Producer-related settings
      producer {
        # Buffer for pushed events
        buffer-size = 100
      }
    }
  }

  snapshot-store {
    class = "com.wavesplatform.matcher.MatcherSnapshotStore"
    plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
    stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"
    dir = ${waves.matcher.snapshots-directory}
  }
}

akka {
  # Without this option, Matcher can't make snapshots.
  # For better support of shutdown process, implement an additional step in a coordinated shutdown:
  # https://doc.akka.io/docs/akka/2.5/actors.html?language=scala#coordinated-shutdown
  jvm-shutdown-hooks = off

  actor {
    allow-java-serialization = off
    guardian-supervisor-strategy = "com.wavesplatform.actor.RootActorSystem$EscalatingStrategy"
    serializers.matcher = "com.wavesplatform.matcher.model.EventSerializers"
    serialization-bindings {
      "com.wavesplatform.matcher.model.Events$Event" = matcher
      "com.wavesplatform.matcher.market.OrderBookActor$Snapshot" = matcher
      "com.wavesplatform.matcher.market.MatcherActor$OrderBookCreated" = matcher
      "com.wavesplatform.matcher.market.MatcherActor$Snapshot" = matcher
    }
  }

  persistence {
    journal {
      plugin = akka.persistence.journal.leveldb
      leveldb {
        dir = ${waves.matcher.journal-directory}
        native = on
      }
    }
    snapshot-store.plugin = waves.matcher.snapshot-store
  }

  kafka {
    consumer {
      # Tuning property of scheduled polls.
      # Controls the interval from one scheduled poll to the next.
      poll-interval = 30ms

      # Tuning property of the `KafkaConsumer.poll` parameter.
      # Note that non-zero value means that the thread that
      # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
      poll-timeout = 100ms

      # The stage will await outstanding offset commit requests before
      # shutting down, but if that takes longer than this timeout it will
      # stop forcefully.
      stop-timeout = 30s

      # Duration to wait for `KafkaConsumer.close` to finish.
      close-timeout = 20s

      # If offset commit requests are not completed within this timeout
      # the returned Future is completed `CommitTimeoutException`.
      commit-timeout = 15s

      # If commits take longer than this time a warning is logged
      commit-time-warning = 1s

      # If for any reason `KafkaConsumer.poll` blocks for longer than the configured
      # poll-timeout then it is forcefully woken up with `KafkaConsumer.wakeup`.
      # The KafkaConsumerActor will throw
      # `org.apache.kafka.common.errors.WakeupException` which will be ignored
      # until `max-wakeups` limit gets exceeded.
      wakeup-timeout = 1m

      # After exceeding maximum wakeups the consumer will stop and the stage will fail.
      # Setting it to 0 will let it ignore the wakeups and try to get the polling done forever.
      max-wakeups = 0

      # If set to a finite duration, the consumer will re-send the last committed offsets periodically
      # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
      commit-refresh-interval = infinite

      # If enabled, log stack traces before waking up the KafkaConsumer to give
      # some indication why the KafkaConsumer is not honouring the `poll-timeout`
      wakeup-debug = false

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the KafkaConsumerActor. Some blocking may occur.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
      # can be defined in this configuration section.
      kafka-clients {
        bootstrap.servers = ""
        group.id = "0"
        auto.offset.reset = "earliest"
        enable.auto.commit = false
        session.timeout.ms = 10000
        max.poll.interval.ms = 11000
        max.poll.records = 100 # Should be <= ${waves.matcher.events-queue.kafka.consumer.buffer-size}
      }

      # Time to wait for pending requests when a partition is closed
      wait-close-partition = 500ms

      # Limits the query to Kafka for a topic's position
      position-timeout = 5s

      # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
      # call to Kafka's API
      offset-for-times-timeout = 5s

      # Timeout for akka.kafka.Metadata requests
      # This value is used instead of Kafka's default from `default.api.timeout.ms`
      # which is 1 minute.
      metadata-request-timeout = 1m
    }

    producer {
      # Tuning parameter of how many sends that can run in parallel.
      parallelism = 100

      # Duration to wait for `KafkaConsumer.close` to finish.
      close-timeout = 60s

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the producer stages. Some blocking may occur.
      # When this value is empty, the dispatcher configured for the stream
      # will be used.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
      eos-commit-interval = 100ms

      # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
      # can be defined in this configuration section.
      kafka-clients {
        bootstrap.servers = ${akka.kafka.consumer.kafka-clients.bootstrap.servers}

        acks = all

        # Buffer messages into a batch for this duration
        linger.ms = 5

        # Maximum size for batch
        batch.size = 16384

        # To guarantee the order
        max.in.flight.requests.per.connection = 1

        compression.type = "none"
      }
    }
  }
}
