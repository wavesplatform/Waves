# Waves node settings in HOCON
# HOCON specification: https://github.com/lightbend/config/blob/master/HOCON.md
waves {
  # Node base directory
  directory = ""
  data-directory = ${waves.directory}"/data"
  # Limits the size of caches which are used during block validation. Lower values slightly decrease memory consummption,
  # while higher values might increase node performance. Setting ghis value to 0 disables caching alltogether.
  max-cache-size = 100000

  max-rollback-depth = 2000

  remember-blocks-interval-in-cache = 3h

  # NTP server
  ntp-server = "pool.ntp.org"

  # P2P Network settings
  network {
    # Peers and blacklist storage file
    file = ${waves.directory}"/peers.dat"

    # String with IP address and port to send as external address during handshake. Could be set automatically if UPnP
    # is enabled.
    #
    # If `declared-address` is set, which is the common scenario for nodes running in the cloud, the node will just
    # listen to incoming connections on `bind-address:port` and broadcast its `declared-address` to its peers. UPnP
    # is supposed to be disabled in this scenario.
    #
    # If declared address is not set and UPnP is not enabled, the node will not listen to incoming connections at all.
    #
    # If declared address is not set and UPnP is enabled, the node will attempt to connect to an IGD, retrieve its
    # external IP address and configure the gateway to allow traffic through. If the node succeeds, the IGD's external
    # IP address becomes the node's declared address.
    #
    # In some cases, you may both set `decalred-address` and enable UPnP (e.g. when IGD can't reliably determine its
    # external IP address). In such cases the node will attempt to configure an IGD to pass traffic from external port
    # to `bind-address:port`. Please note, however, that this setup is not recommended.
    # declared-address = "1.2.3.4:6863"

    # Network address
    bind-address = "0.0.0.0"

    # Port number
    port = 6863

    # Node name to send during handshake. Comment this string out to set random node name.
    # node-name = "default-node-name"

    # Node nonce to send during handshake. Should be different if few nodes runs on the same external IP address. Comment this out to set random nonce.
    # nonce = 0

    # List of IP addresses of well known nodes.
    known-peers = ["52.30.47.67:6863", "52.28.66.217:6863", "52.77.111.219:6863", "52.51.92.182:6863"]

    # How long the information about peer stays in database after the last communication with it
    peers-data-residence-time = 1d

    # How long peer stays in blacklist after getting in it
    black-list-residence-time = 15m

    # Breaks a connection if there is no message from the peer during this timeout
    break-idle-connections-timeout = 5m

    # How many network inbound network connections can be made
    max-inbound-connections = 30

    # Number of outbound network connections
    max-outbound-connections = 30

    # Number of connections from single host
    max-single-host-connections = 3

    # Timeout on network communication with other peers
    connection-timeout = 30s

    # Size of circular buffer to store unverified (not properly handshaked) peers
    max-unverified-peers = 100

    # If yes the node requests peers and sends known peers
    enable-peers-exchange = yes

    # If yes the node can blacklist others
    enable-blacklisting = yes

    # How often connected peers list should be broadcasted
    peers-broadcast-interval = 2m

    # When accepting connection from remote peer, this node will wait for handshake for no longer than this value. If
    # remote peer fails to send handshake within this interval, it gets blacklisted. Likewise, when connecting to a
    # remote peer, this node will wait for handshake response for no longer than this value. If remote peer does not
    # respond in a timely manner, it gets blacklisted.
    handshake-timeout = 30s

    suspension-residence-time = 1m

    # When a new treansaction comes from the network, we cache it and doesn't push this transaction again when it comes
    # from another peer.
    # This setting setups a timeout to remove an expired transaction in the elimination cache.
    received-txs-cache-timeout = 3m

    upnp {
      # Enable UPnP tunnel creation only if you router/gateway supports it. Useful if your node is runnin in home
      # network. Completely useless if you node is in cloud.
      enable = no

      # UPnP timeouts
      gateway-timeout = 7s
      discover-timeout = 3s
    }

    # Logs incoming and outgoing messages
    traffic-logger {
      # Codes of transmitted messages to ignore. See MessageSpec.messageCode
      ignore-tx-messages = [23, 25] # BlockMessageSpec, TransactionMessageSpec

      # Codes of received messages to ignore. See MessageSpec.messageCode
      ignore-rx-messages = [25] # TransactionMessageSpec
    }
  }

  # Wallet settings
  wallet {
    # Path to wallet file
    file = ${waves.directory}"/wallet/wallet.dat"

    # Password to protect wallet file
    # password = "some string as password"

    # The base seed, not an account one!
    # By default, the node will attempt to generate a new seed. To use a specific seed, uncomment the following line and
    # specify your base58-encoded seed.
    # seed = "BASE58SEED"
  }

  # Blockchain settings
  blockchain {
    # Blockchain type. Could be TESTNET | MAINNET | CUSTOM. Default value is TESTNET.
    type = TESTNET

    # 'custom' section present only if CUSTOM blockchain type is set. It's impossible to overwrite predefined 'testnet' and 'mainnet' configurations.
    #    custom {
    #      # Address feature character. Used to prevent mixing up addresses from different networks.
    #      address-scheme-character = "C"
    #
    #      # Timestamps/heights of activation/deactivation of different functions.
    #      functionality {
    #
    #        # Blocks period for feature checking and activation
    #        feature-check-blocks-period = 10000
    #
    #        # Blocks required to accept feature
    #        blocks-for-feature-activation = 9000
    #
    #        allow-temporary-negative-until = 0
    #        allow-invalid-payment-transactions-by-timestamp = 0
    #        generation-balance-depth-from-50-to-1000-after-height = 0
    #        minimal-generating-balance-after = 0
    #        allow-transactions-from-future-until = 0
    #        allow-unissued-assets-until = 0
    #        block-version-3-after-height = 0
    #        pre-activated-features {
    #          1 = 100
    #          2 = 200
    #        }
    #        max-transaction-time-back-offset = 120m
    #        max-transaction-time-forward-offset = 90m
    #      }
    #
    #      # List of genesis transactions
    #      genesis {
    #        # Timestamp of genesis block and transactions in it
    #        timestamp = 1460678400000
    #
    #        # Genesis block signature
    #        signature = "BASE58BLOCKSIGNATURE"
    #
    #        # Initial balance in smallest units
    #        initial-balance = 100000000000000
    #
    #        # Initial base target
    #        initial-base-target =153722867
    #
    #        # Average delay between blocks
    #        average-block-delay = 60s
    #
    #        # List of genesis transactions
    #        transactions = [
    #          {recipient = "BASE58ADDRESS1", amount = 50000000000000},
    #          {recipient = "BASE58ADDRESS2", amount = 50000000000000}
    #        ]
    #      }
    #    }
  }

  # Matcher settings
  matcher {
    # Enable/disable matcher
    enable = no

    # Matcher's account address
    account = ""

    # Matcher REST API bind address
    bind-address = "127.0.0.1"

    # Matcher REST API port
    port = 6886

    # Timeout for REST API responses from actors.
    # To change a timeout for all REST API responses, change this option and akka.http.server.request-timeout
    actor-response-timeout = ${akka.http.server.request-timeout}

    # Settings for matcher's fee in order
    order-fee {

      # Standart fee in waves or fixed asset and fee or percent fee in asset of the pair
      mode = "waves" # | "fixed" | "percent"

      # Has additional change for matcher and assets scripts
      waves {
        # Absolute
        base-fee = 300000
      }

      fixed {
        # Fixed fee asset
        asset = "WAVES" # | "some issued asset (base58)"

        # Minimum allowed order fee for fixed mode
        min-fee = 300000
      }

      percent {
        # Asset type for fee
        asset-type = "amount" # | "price" | "spending" | "receiving"

        # In percents
        min-fee = 0.1
      }
    }

    # Price and fee deviations (in percents)
    max-price-deviations {
      # Enable/disable deviations checks
      enable = no
      # Max price deviation IN FAVOR of the client
      profit = 1000000
      # Max price deviation AGAINST the client
      loss = 1000000
      # Max fee deviation from the market price
      fee = 1000000
    }

    # Whitelist of the asset pairs. Empty whitelist means that all pairs are allowed
    #
    # allowed-asset-pairs = [
    #  "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS"
    # ]
    allowed-asset-pairs = []

    # Enable/disable orders of version 3
    allow-order-v3 = no

    # Restrictions for orders amounts. Empty list means that there are no restrictions on order's amount
    #
    # order-amount-restrictions = [
    #   {
    #     pair = "WAVES-8LQW8f7P5d5PZM7GtZEBgaqRPGSzS3DfPuiXrURJ4AJS",
    #     min-amount = 0.001,
    #     max-amount = 1000000
    #     step-size = 0.001
    #   },
    #   ...
    # ]
    order-amount-restrictions = []

    # Disbale charging of additional fee for new orders from scripted accounts or for smart assets
    disable-extra-fee-for-script = no

    # Matcher's directories
    matcher-directory = ${waves.directory}"/matcher"
    data-directory = ${waves.matcher.matcher-directory}"/data"
    journal-directory = ${waves.matcher.matcher-directory}"/journal"
    snapshots-directory = ${waves.matcher.matcher-directory}"/snapshots"

    # Snapshots creation interval (in events)
    snapshots-interval = 1000000

    # Make snapshots after recovery at start
    make-snapshots-at-start = no

    # Maximum time to recover all order books from snapshots
    snapshots-loading-timeout = 10m

    # Maximum time to recover events those observed at start
    start-events-processing-timeout = 20m

    # Maximum allowed amount of orders retrieved via REST
    rest-order-limit = 100

    # Base assets used as price assets
    price-assets: []

    # Blacklisted assets id
    blacklisted-assets: []

    # Blacklisted assets name
    blacklisted-names: []

    # Blacklisted addresses
    blacklisted-addresses: []

    # Cache for /matcher/orderbook/{amountAsset}/{priceAsset}?depth=N
    order-book-snapshot-http-cache {
      # A timeout to store cache
      cache-timeout = 5s

      # Cache for these depths. When ?depth=3 is requested, returned a cache for depth of 10
      depth-ranges = [10, 100]
    }

    # Interval to buffer balance changes before process them
    balance-watching-buffer-interval = 5s

    # Queue for events (order was added, order was cancelled)
    events-queue {
      # Store events locally in LevelDB
      type = "local" # Other possible values: kafka

      local {
        # Interval between reads from the disk
        polling-interval = 20ms

        # Max elements per poll
        max-elements-per-poll = 100

        # Clean old records before start consuming
        clean-before-consume = yes
      }

      kafka {
        # Where events should be written and read from
        topic = "dex-events"

        # Consumer-related settings
        consumer {
          # Buffer for polled events
          buffer-size = 100

          # Recovery settings (see akka.stream.scaladsl.Restart for detailed information)

          # Initial duration before consumer start after fail
          min-backoff = 3s

          # Maximum duration before consumer start after fail
          max-backoff = 10s
        }

        # Producer-related settings
        producer {
          # Buffer for pushed events
          buffer-size = 100
        }
      }
    }
  }

  # New blocks generator settings
  miner {
    # Enable/disable block generation
    enable = yes

    # Required number of connections (both incoming and outgoing) to attempt block generation. Setting this value to 0
    # enables "off-line generation".
    quorum = 1

    # Enable block generation only in the last block is not older the given period of time
    interval-after-last-block-then-generation-is-allowed = 1d

    # Mining attempts delay, if no quorum available
    no-quorum-mining-delay = 5s

    # Interval between microblocks
    micro-block-interval = 5s

    # Max amount of transactions in key block
    max-transactions-in-key-block = 0

    # Max amount of transactions in micro block
    max-transactions-in-micro-block = 255

    # Miner references the best microblock which is at least this age
    min-micro-block-age = 6s
  }

  # Node's REST API settings
  rest-api {
    # Enable/disable REST API
    enable = yes

    # Network address to bind to
    bind-address = "127.0.0.1"

    # Port to listen to REST API requests
    port = 6869

    # Hash of API key string
    api-key-hash = "H6nsiifwYKYEx6YzYD7woP1XCn72RVvx6tC1zjjLXqsu"

    # Enable/disable CORS support
    cors = yes

    # Enable/disable X-API-Key from different host
    api-key-different-host = no

    # Max number of transactions
    # returned by /transactions/address/{address}/limit/{limit}
    transactions-by-address-limit = 10000
    distribution-address-limit = 1000
  }

  # Nodes synchronization settings
  synchronization {

    # How many blocks could be rolled back if fork is detected. If fork is longer than this rollback is impossible.
    max-rollback = 100

    # I don't know
    max-chain-length = 101

    # Timeout to receive all requested blocks
    synchronization-timeout = 60s

    # Time to live for broadcasted score
    score-ttl = 90s

    # Max baseTarget value. Stop node when baseTraget greater than this param. No limit if it is not defined.
    # max-base-target = 200

    # Settings for invalid blocks cache
    invalid-blocks-storage {
      # Maximum elements in cache
      max-size = 30000

      # Time to store invalid blocks and blacklist their owners in advance
      timeout = 5m
    }

    # History replier caching settings
    history-replier {
      # Max microblocks to cache
      max-micro-block-cache-size = 50

      # Max blocks to cache
      max-block-cache-size = 20
    }

    # Utx synchronizer caching settings
    utx-synchronizer {
      # Max microblocks to cache
      network-tx-cache-size = 1000000

      # Max number of transactions in buffer. When the limit is reached, the node processes all transactions in batch
      max-buffer-size = 500

      # Max time for buffer. When time is out, the node processes all transactions in batch
      max-buffer-time = 100ms

      # Max scheduler parallelism
      parallelism = 4

      # Max scheduler threads
      max-threads = 8

      # Max pending queue size
      max-queue-size = 5000
    }

    # MicroBlock synchronizer settings
    micro-block-synchronizer {
      # How much time to wait before a new request of a microblock will be done
      wait-response-timeout = 2s

      # How much time to remember processed microblock signatures
      processed-micro-blocks-cache-timeout = 3m

      # How much time to remember microblocks and their nodes to prevent same processing
      inv-cache-timeout = 45s
    }
  }

  # Unverified transactions pool settings
  utx {
    # Pool size
    max-size = 100000
    # Pool size in bytes
    max-bytes-size = 52428800 // 50 MB
    # Pool size for scripted transactions
    max-scripted-size = 5000
    # Blacklist transactions from these addresses (Base58 strings)
    blacklist-sender-addresses = []
    # Allow transfer transactions from the blacklisted addresses to these recipients (Base58 strings)
    allow-blacklisted-transfer-to = []
    # Allow transactions from smart accounts
    allow-transactions-from-smart-accounts = true
    # Allow skipping checks with highest fee
    allow-skip-checks = true
  }

  features {
    auto-shutdown-on-unsupported-feature = yes
    supported = []
  }
}

# Performance metrics
kamon {
  # Set to "yes", if you want to report metrics
  enable = no

  # A node identification
  environment {
    service = "waves-node"

    # An unique id of your node to distinguish it from others
    # host = ""
  }

  # An interval within metrics are aggregated. After it, them will be sent to the server
  metric.tick-interval = 10 seconds

  # Reporter settings
  influxdb {
    hostname = "127.0.0.1"
    port = 8086
    database = "mydb"

    # authentication {
    #   user = ""
    #   password = ""
    # }
  }
}

# Non-aggregated data (information about blocks, transactions, ...)
metrics {
  enable = no
  node-id = -1 # ${kamon.environment.host}

  influx-db {
    uri = "http://"${kamon.influxdb.hostname}":"${kamon.influxdb.port}
    db = ${kamon.influxdb.database}

    # username = ${kamon.influxdb.authentication.user}
    # password = ${kamon.influxdb.authentication.password}

    batch-actions = 100
    batch-flash-duration = 5s
  }
}

# WARNING: No user-configurable settings below this line.

waves.matcher.snapshot-store {
  class = "com.wavesplatform.matcher.MatcherSnapshotStore"
  plugin-dispatcher = "akka.persistence.dispatchers.default-plugin-dispatcher"
  stream-dispatcher = "akka.persistence.dispatchers.default-stream-dispatcher"
  dir = ${waves.matcher.snapshots-directory}
}

akka {
  loglevel = "INFO"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters-during-shutdown = false

  # Without this option, Matcher can't make snapshots.
  # For better support of shutdown process, implement an additional step in a coordinated shutdown:
  # https://doc.akka.io/docs/akka/2.5/actors.html?language=scala#coordinated-shutdown
  jvm-shutdown-hooks = off

  actor {
    allow-java-serialization = off
    guardian-supervisor-strategy = "com.wavesplatform.actor.RootActorSystem$EscalatingStrategy"
    serializers.matcher = "com.wavesplatform.matcher.model.EventSerializers"
    serialization-bindings {
      "com.wavesplatform.matcher.market.OrderBookActor$Snapshot" = matcher
      "com.wavesplatform.matcher.market.MatcherActor$OrderBookCreated" = matcher
      "com.wavesplatform.matcher.market.MatcherActor$Snapshot" = matcher
    }
  }

  http.server {
    max-connections = 128
    request-timeout = 20s
    parsing {
      max-method-length = 64
      max-content-length = 1m
    }
  }

  io.tcp {
    direct-buffer-size = 1536 KiB
    trace-logging = off
  }

  persistence {
    journal {
      plugin = akka.persistence.journal.leveldb
      leveldb {
        dir = ${waves.matcher.journal-directory}
        native = on
      }
    }
    snapshot-store.plugin = waves.matcher.snapshot-store
  }

  kafka {
    default-dispatcher.thread-pool-executor.fixed-pool-size = 4

    consumer {
      # Tuning property of scheduled polls.
      # Controls the interval from one scheduled poll to the next.
      poll-interval = 30ms

      # Tuning property of the `KafkaConsumer.poll` parameter.
      # Note that non-zero value means that the thread that
      # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
      poll-timeout = 100ms

      # The stage will await outstanding offset commit requests before
      # shutting down, but if that takes longer than this timeout it will
      # stop forcefully.
      stop-timeout = 30s

      # Duration to wait for `KafkaConsumer.close` to finish.
      close-timeout = 20s

      # If offset commit requests are not completed within this timeout
      # the returned Future is completed `CommitTimeoutException`.
      commit-timeout = 15s

      # If commits take longer than this time a warning is logged
      commit-time-warning = 1s

      # If for any reason `KafkaConsumer.poll` blocks for longer than the configured
      # poll-timeout then it is forcefully woken up with `KafkaConsumer.wakeup`.
      # The KafkaConsumerActor will throw
      # `org.apache.kafka.common.errors.WakeupException` which will be ignored
      # until `max-wakeups` limit gets exceeded.
      wakeup-timeout = 1m

      # After exceeding maximum wakeups the consumer will stop and the stage will fail.
      # Setting it to 0 will let it ignore the wakeups and try to get the polling done forever.
      max-wakeups = 0

      # If set to a finite duration, the consumer will re-send the last committed offsets periodically
      # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
      commit-refresh-interval = infinite

      # If enabled, log stack traces before waking up the KafkaConsumer to give
      # some indication why the KafkaConsumer is not honouring the `poll-timeout`
      wakeup-debug = false

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the KafkaConsumerActor. Some blocking may occur.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
      # can be defined in this configuration section.
      kafka-clients {
        bootstrap.servers = ""
        group.id = "0"
        auto.offset.reset = "earliest"
        enable.auto.commit = false
        session.timeout.ms = 10000
        max.poll.interval.ms = 11000
        max.poll.records = 100 # Should be <= ${waves.matcher.events-queue.kafka.consumer.buffer-size}
      }

      # Time to wait for pending requests when a partition is closed
      wait-close-partition = 500ms

      # Limits the query to Kafka for a topic's position
      position-timeout = 5s

      # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
      # call to Kafka's API
      offset-for-times-timeout = 5s

      # Timeout for akka.kafka.Metadata requests
      # This value is used instead of Kafka's default from `default.api.timeout.ms`
      # which is 1 minute.
      metadata-request-timeout = 20s
    }

    producer {
      # Tuning parameter of how many sends that can run in parallel.
      parallelism = 100

      # Duration to wait for `KafkaConsumer.close` to finish.
      close-timeout = 60s

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the producer stages. Some blocking may occur.
      # When this value is empty, the dispatcher configured for the stream
      # will be used.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
      eos-commit-interval = 100ms

      # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
      # can be defined in this configuration section.
      kafka-clients {
        bootstrap.servers = ${akka.kafka.consumer.kafka-clients.bootstrap.servers}

        acks = all

        # Buffer messages into a batch for this duration
        linger.ms = 5

        # Maximum size for batch
        batch.size = 16384

        # To guarantee the order
        max.in.flight.requests.per.connection = 1

        compression.type = "none"
      }
    }
  }
}
